import sqlite3
import requests
from bs4 import BeautifulSoup
import schedule
import time

TOP_URL = "https://sploitus.com/top"

def parse_exploit(exploit_id):
    exploit_url = f"https://sploitus.com/exploit?id={exploit_id}"
    response = requests.get(exploit_url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "html.parser")

    subtitle_element = soup.find("div", class_="tile-subtitle text-gray")
    publication_date = subtitle_element.text.strip() if subtitle_element else "Дата публикации отсутствует."

    return publication_date, exploit_url

def init_db():
    conn = sqlite3.connect("lastcve.db")
    cursor = conn.cursor()

    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS vulnerabilities (
            id TEXT PRIMARY KEY,
            title TEXT,
            publication_date TEXT,
            url TEXT
        )
        """
    )
    conn.commit()
    conn.close()

def save_to_db(vulnerability):
    conn = sqlite3.connect("lastcve.db")
    cursor = conn.cursor()

    cursor.execute(
        """
        INSERT OR IGNORE INTO vulnerabilities (id, title, publication_date, url)
        VALUES (?, ?, ?, ?)
        """,
        vulnerability
    )

    conn.commit()
    conn.close()

def fetch_and_save():
    response = requests.get(TOP_URL)
    response.raise_for_status()
    data = response.json()

    for exploit in data.get("top", []):
        exploit_id = exploit.get("id")
        exploit_title = exploit.get("title")

        if not exploit_id or not exploit_title:
            continue

        publication_date, exploit_url = parse_exploit(exploit_id)

        print("#########################")
        print(f"ID уязвимости: {exploit_id}")
        print(f"Название: {exploit_title}")
        print(f"Дата публикации: {publication_date}")
        print(f"Ссылка на источник: {exploit_url}")
        print("#########################\n")

        save_to_db((exploit_id, exploit_title, publication_date, exploit_url))

    print("Данные успешно сохранены в базу данных.")

init_db()

schedule.every(24).hours.do(fetch_and_save)

while True:
    schedule.run_pending()
    time.sleep(60)
